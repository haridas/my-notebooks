{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the posterior of coin fairness using SVI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Pyro Representations\n",
    "\n",
    "When representing the different entities in the probabilistic programming like\n",
    "observations, latent variables and parameters use the below functions.\n",
    "\n",
    "1. Observations -> `pyro.sample` with `obs` argument \n",
    "2. Latent Variables -> `pyro.sample` \n",
    "3. Parameters -> `pyro.param`\n",
    "\n",
    "**Model** is the posterior or hidden random variables $p_{\\theta}(z|x)$\n",
    "\n",
    "**Guide Function** in `pyro` is the Variation Distribution $q_{\\phi}({\\bf x})$\n",
    "\n",
    "`guide` is the approximation of the posterior $p_{\\theta}(z|x)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**TEST**\n",
    "1. $p_{\\theta}({\\bf x}, {\\bf z}) = p_{\\theta}({\\bf x}|{\\bf z}) p_{\\theta}({\\bf z})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin fairness estimation using SVI\n",
    "\n",
    "SVI - Stocastic Variational Inference\n",
    "\n",
    "\n",
    "If you get a coin and want to know whether it's biased or not, how can you do that ?\n",
    "\n",
    "1. Do some trial by doing tossing the coin and measure the observations, N number of times.\n",
    "2. Pick a prior distribution, we can assume the coin is fair one. We can pick a Beta(10, 10), as it's centered around 0.5.\n",
    "\n",
    "\n",
    "### Model the problem\n",
    "\n",
    "Head and Tail are encoded as set of two valuesSet(Head(1), Tail(0))\n",
    "\n",
    "A Random variable $X$ is encoded as fairness of the coin.  The random variable  $X$ takes real value from -> [0, 1] based on the fairness of the coin. We don't know the current distribution of Random Variable $X$, we assume ideally it has $Beta(20, 20)$ as it prior distribution. We are picking $Beta(10, 10) because its expected value comes at `0.5`.\n",
    "\n",
    "The coin is Fair if we get equal number of head and tail on tossing.\n",
    "\n",
    "So we have to do experiment/Trail using this coin and measure the observation and then validate the current fairness of the coin based on the observation that we got from the experiment. Use the prior distribution of the Fairness as above. \n",
    "\n",
    "\n",
    "#### Graphical Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.sample(\"obs\", pyro.distributions.Bernoulli(0.5), obs=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    \"\"\"\n",
    "    Using the Likelihood and Prior estimate the Posterior.\n",
    "    \n",
    "    P(z|X;𝜃) = P(X|z;𝜃) * P(z;𝜃)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hyper parameters for the prior distribution, prior belief about\n",
    "    # the fairness.\n",
    "    alpha0 = torch.tensor(10.)\n",
    "    beta0 = torch.tensor(10.)\n",
    "    \n",
    "    # Sample Prior beleif about the coin or P(x_fair) from Beta prior.\n",
    "    x_prior = pyro.sample(\"latent_fairness\", pyro.distributions.Beta(alpha0, beta0))\n",
    "    \n",
    "    # Create the Observations from the data / trails we done using the\n",
    "    # given coin.\n",
    "    for ind, d in enumerate(data):\n",
    "        # Observed datapoint from the Bernoulli.\n",
    "        # Likelihood is Bernoulli(x_prior) or P(Data|x_prior)\n",
    "        pyro.sample(f\"obs_{ind}\", pyro.distributions.Bernoulli(x_prior), obs=torch.tensor(d))\n",
    "    \n",
    "\n",
    "def guide(data):\n",
    "    \"\"\"\n",
    "    A Variational Distribution on Latent variable \"latent_fairness\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # We don't know the distribution of the variational distribution, \n",
    "    # we need to find best parameters of that distribution to match the\n",
    "    # posterior distribution.\n",
    "    \n",
    "    # Let's pick another Beta Distribution, so both prior and posterior are\n",
    "    # Compatible or conjugate to each other.\n",
    "    \n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.),\n",
    "                         constraint=pyro.distributions.constraints.positive)\n",
    "    \n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.),\n",
    "                       constraint=pyro.distributions.constraints.positive)\n",
    "    \n",
    "    pyro.sample(\"latent_fairness\", pyro.distributions.Beta(alpha_q, beta_q))\n",
    "    \n",
    "    \n",
    "def sample_data():\n",
    "    return [torch.tensor(1.)] * 1 + [torch.tensor(0.)] * 300\n",
    "\n",
    "\n",
    "def learn_posterior(data, model, guide, n_steps=5000):\n",
    "    \"\"\"\n",
    "    Learn the posterior from the observation. Or update our prior\n",
    "    information about the latent variable is correct or not.\n",
    "    \"\"\"\n",
    "    # Validate the params are from the right distribution.\n",
    "    pyro.enable_validation(True)\n",
    "    \n",
    "    # Clear the param store, before starting the training.\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Optimizer settings.\n",
    "    adam_param = {\"lr\": 0.005, \"betas\":(0.9, 0.999) }\n",
    "    optimizer = Adam(adam_param)\n",
    "    \n",
    "    # Inference algorithm setup.\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "    \n",
    "    \n",
    "    # Do inference from observation using SVI.\n",
    "    for i in range(n_steps):\n",
    "        svi.step(data)\n",
    "        if i % 100 == 0:\n",
    "            print (\".\", end=\"\")\n",
    "    \n",
    "    # Now we have updated parameters that we asked to learn in guide.\n",
    "    # Or we found a best variational distribution by picking best\n",
    "    # parameters, lets pick the parameter and get some properties of the\n",
    "    # variational distribution.\n",
    "    #\n",
    "    alpha_q = pyro.param(\"alpha_q\").item()\n",
    "    beta_q  = pyro.param(\"beta_q\").item()\n",
    "    \n",
    "    infered_mean = alpha_q / (alpha_q + beta_q)\n",
    "    factor = beta_q / (alpha_q * (1. + alpha_q + beta_q))\n",
    "    infered_std = infered_mean * math.sqrt(factor)\n",
    "    \n",
    "    print()\n",
    "    print(f\"Posterior parameters are -> Beta({alpha_q}, {beta_q})\")\n",
    "    print(f\"\\nBased on the Data and the Prior distribution, fairness of the \" +\n",
    "          f\"coin is {infered_mean : 0.3f} +- {infered_std : 0.3f}\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super(Model, self).__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/ENV3/lib/python3.6/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample_data()\n",
    "# sum(data)\n",
    "\n",
    "torch.nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haridas/ENV3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................\n",
      "Posterior parameters are -> Beta(5.59282922744751, 152.99102783203125)\n",
      "\n",
      "Based on the Data and the Prior distribution, fairness of the coin is  0.035 +-  0.015\n"
     ]
    }
   ],
   "source": [
    "data = sample_data()\n",
    "\n",
    "learn_posterior(data, model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn_posterior(data, model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn_posterior(data, model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn_posterior(data, model, guide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
